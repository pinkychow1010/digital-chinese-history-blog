{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chinese_Keyword_Extraction_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \"Combining Web Scraping with Keyword Extraction\"\n",
        "> \"Chinese Keyword Extraction using Jieba (II)\"\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [level-4, chapter-4, jieba, BeautifulSoup, web-scraping]\n",
        "- image: images/tunnel.jpg"
      ],
      "metadata": {
        "id": "nCmIvi0Kmlbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keyword extraction is one of the very popular techniques in Natural Language Processing (NLP) and text analysis. [Last time](https://pinkychow1010.github.io/digital-chinese-history-blog/level-4/chapter-4/jieba/text-mining/2022/02/05/Chinese_Keyword_Extraction_1.html) we learnt about how to extract keywords from Chinese text using **Jieba**, this time we will learn how to extract keywords directly from the web using **web scraping technique**. It can be achieved using **BeautifulSoup**, a Python library for pulling data out of HTML and XML files. What is web scraping? Web scraping is an automated process used to download the page (fetching) and copy data from the web. Examples include copying a table or book titles from a website.\n",
        "\n",
        "In this lesson, we will download the Chinese blog **时差播客︱宗教学：信仰，魔法，身份，权力** from **澎湃新闻** and extract keywords from the content. You will also learn how to do it with any website you want.\n",
        "\n",
        "> **IMPORTANT**: \n",
        ">\n",
        "> As mentioned in the [instructions](https://pinkychow1010.github.io/digital-chinese-history-blog/about/), you can click on the icon **\"open in Colab\"** to open the script in a **Jupyter notebook** to run the code. It is highly recommended to follow the tutorials in the correct order. \n",
        "\n",
        "# Set Up Environment 🌲 \n",
        "\n",
        "First, we have to set up our cloud environment in **[Colab](https://pinkychow1010.github.io/digital-chinese-history-blog/level-1/chapter-1/jupyter/colab/2020/01/30/JupyterNotebook_Colab_Basics.html)**."
      ],
      "metadata": {
        "id": "j-BRrs3flzu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Library\n",
        "\n",
        "* Download Library\n",
        "\n",
        "We need to download **Jieba** using **pip**."
      ],
      "metadata": {
        "id": "lnWcirdkeu3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install jieba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKvv9f8AgxoT",
        "outputId": "441eca2a-bc10-4f2a-a8a1-1da399e74ed5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (0.42.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import Libraries\n",
        "\n",
        "We will then import **Jieba**, **BeautifulSoup** and other libraries we need. 📚"
      ],
      "metadata": {
        "id": "2pn42IPjew4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "# Jieba for tokenization and keyword extraction\n",
        "import jieba\n",
        "import jieba.posseg\n",
        "import jieba.analyse\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage import filters\n",
        "import time\n",
        "\n",
        "# Open URL\n",
        "from urllib.request import urlopen, Request\n",
        "import ssl\n",
        "\n",
        "# Web scarping\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "GifW9jvKrTTJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive\n",
        " \n",
        "* Connect to Google Drive\n",
        "\n",
        "To access resources in your own Google Drive, we need to permit it by running the following code."
      ],
      "metadata": {
        "id": "kly_Lk-Key8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9dM6BimsnGS",
        "outputId": "6cfc62dc-cc96-40f5-d30b-a96d7a8500c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Resources using wget\n",
        "\n",
        "In this lesson, there are two materials we need to download from the web. The first one is the Chinese font which we need to display characters in the plot. The second one is a list of Chinese stopwords which we need for tokenization. We can access both of them using **wget**.\n",
        "\n",
        "* Download Chinese Font"
      ],
      "metadata": {
        "id": "_7BUDFkOe0sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download Chinese font\n",
        "!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "\n",
        "# after download, we have to add the font into the plotting library\n",
        "# we need matplotlib.font_manager for that\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import fontManager\n",
        "\n",
        "fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')\n",
        "mpl.rc('font', family='Taipei Sans TC Beta')"
      ],
      "metadata": {
        "id": "0-uuHPvjj7cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download Chinese Stopword List\n",
        "\n",
        "From the github link, we can access the list of stopwords."
      ],
      "metadata": {
        "id": "EJkOSmIUe3BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://github.com/stopwords-iso/stopwords-zh/blob/master/stopwords-zh.txt -P /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "2PD9Uvmx3rIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping Basics 🌱\n",
        "\n",
        "We have learnt how to extract keywords from strings. What if this time, we do not want to copy the whole text, but directly get the text from the web? It can be done by directly scrapping the text from URLs using **BeautifulSoup** 🥣. \n",
        "\n",
        "The function BeautifulSoup from the library can parse the HTML code to Python objects. **Data parsing** is a process in which a string of data is converted from one format to another. To start with, we need to pass the web address to variable `url`, then open `url` using **urllib.request** and convert the code with `\"html.parser\"`. We will get the `soup` at the end. \n",
        "\n",
        "In order to extract only text for our analysis, we will remove the HTML tags using **extract()**, following by **get_text()**. To further exclude irrelevant texts from the headers, we can choose to select only the blog content by specifying the **class** of the content using **find()**. To find out the class, we can go to the webpage, open the **developer tool** and use the **inspector** to click on the blog content. We will then find out, the class we need is called **\"newsdetail_content\"**.\n",
        "\n",
        "To guide you through each step of the process, we will first get the text without filtering the data."
      ],
      "metadata": {
        "id": "gwYPieP1GUHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1:** Get the HTML without parsing.\n",
        "\n",
        "First, we will solely read the HTML code from the URL. We can see the result includes not only text but also HTML code. All the Chinese characters are also displayed in **UTF-8**."
      ],
      "metadata": {
        "id": "rL03-QXpsPRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the link using request being sent from the Firefox web browser\n",
        "url = \"https://m.thepaper.cn/newsDetail_forward_13762466\"\n",
        "\n",
        "# this line is needed to avoid running into HTTP error 403 (access denial because of security)\n",
        "req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "html = urlopen(req).read()\n",
        "\n",
        "print(html[500:1000])"
      ],
      "metadata": {
        "id": "0AFUtvcLzVnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787a00f9-bb7f-4f65-83c6-59d269cb3851"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'-touch-fullscreen\"/>\\n<meta name=\"Keywords\" content=\"\\xe6\\xbe\\x8e\\xe6\\xb9\\x83\\xef\\xbc\\x8cPaper\\xef\\xbc\\x8cThe Paper\\xef\\xbc\\x8c\\xe7\\x83\\xad\\xe9\\x97\\xae\\xe7\\xad\\x94\\xef\\xbc\\x8c\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xe8\\xb7\\x9f\\xe8\\xb8\\xaa\\xef\\xbc\\x8c\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xef\\xbc\\x8c\\xe6\\x97\\xb6\\xe6\\x94\\xbf\\xef\\xbc\\x8c\\xe6\\x94\\xbf\\xe7\\xbb\\x8f\\xef\\xbc\\x8c\\xe6\\xbe\\x8e\\xe6\\xb9\\x83\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xef\\xbc\\x8c\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xef\\xbc\\x8c\\xe6\\x80\\x9d\\xe6\\x83\\xb3\\xef\\xbc\\x8c\\xe5\\x8e\\x9f\\xe5\\x88\\x9b\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xef\\xbc\\x8c\\xe7\\xaa\\x81\\xe5\\x8f\\x91\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xef\\xbc\\x8c\\xe7\\x8b\\xac\\xe5\\xae\\xb6\\xe6\\x8a\\xa5\\xe9\\x81\\x93\\xef\\xbc\\x8c\\xe4\\xb8\\x8a\\xe6\\xb5\\xb7\\xe6\\x8a\\xa5\\xe4\\xb8\\x9a\\xef\\xbc\\x8c\\xe4\\xb8\\x9c\\xe6\\x96\\xb9\\xe6\\x97\\xa9\\xe6\\x8a\\xa5\\xef\\xbc\\x8c\\xe4\\xb8\\x9c\\xe6\\x96\\xb9\\xe6\\x8a\\xa5\\xe4\\xb8\\x9a\\xef\\xbc\\x8c\\xe4\\xb8\\x8a\\xe6\\xb5\\xb7\\xe4\\xb8\\x9c\\xe6\\x96\\xb9\\xe6\\x8a\\xa5\\xe4\\xb8\\x9a\" />\\n<meta name=\"Description\" content=\"\\xe6\\xbe\\x8e\\xe6\\xb9\\x83\\xef\\xbc\\x8c\\xe6\\xbe\\x8e\\xe6\\xb9\\x83\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xef\\xbc\\x8c\\xe6\\xbe\\x8e\\xe6\\xb9\\x83\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xe7\\xbd\\x91\\xef\\xbc\\x8c\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xe4\\xb8\\x8e\\xe6\\x80\\x9d\\xe6\\x83\\xb3\\xef\\xbc\\x8c\\xe6\\xbe\\x8e\\xe6\\xb9\\x83\\xe6\\x98\\xaf\\xe6\\xa4\\x8d\\xe6\\xa0\\xb9\\xe4\\xba\\x8e\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe4\\xb8\\x8a\\xe6\\xb5\\xb7\\xe7\\x9a\\x84\\xe6\\x97\\xb6\\xe6\\x94\\xbf\\xe6\\x80\\x9d\\xe6\\x83\\xb3\\xe7\\xb1\\xbb\\xe4\\xba\\x92\\xe8\\x81\\x94\\xe7\\xbd\\x91\\xe5\\xb9\\xb3\\xe5\\x8f\\xb0\\xef\\xbc\\x8c\\xe4\\xbb\\xa5\\xe6\\x9c\\x80\\xe6\\xb4\\xbb\\xe8\\xb7\\x83\\xe7\\x9a\\x84\\xe5\\x8e\\x9f\\xe5\\x88\\x9b\\xe6\\x96\\xb0\\xe9\\x97\\xbb\\xe4\\xb8\\x8e\\xe6\\x9c\\x80\\xe5\\x86\\xb7\\xe9\\x9d\\x99\\xe7\\x9a\\x84\\xe6\\x80\\x9d\\xe6\\x83\\xb3\\xe5\\x88\\x86\\xe6\\x9e\\x90\\xe4\\xb8\\xba\\xe4\\xb8'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2:** Parse the HTML.\n",
        "\n",
        "After parsing the HTML, the layout gets easier to read. We begin to recognize the Chinese characters, but still with a lot of code."
      ],
      "metadata": {
        "id": "n3SMaLtGTJq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse it to BeautifulSoup\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "type(soup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYn6MsoGx00o",
        "outputId": "e37c6924-9f67-4371-c217-c0abf98d479f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup_string = str(soup)\n",
        "print(soup_string[:1500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVBh9vtxyODs",
        "outputId": "22a98c2c-1f13-4524-acae-790e5a74202a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "\n",
            "<html lang=\"cn\">\n",
            "<head>\n",
            "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
            "<meta content=\"zh-CN\" http-equiv=\"content-language\"/>\n",
            "<meta content=\"initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no,viewport-fit=cover\" name=\"viewport\"/>\n",
            "<meta content=\"no\" name=\"apple-mobile-web-app-capable\"/>\n",
            "<meta content=\"black\" name=\"apple-mobile-web-app-status-bar-style\">\n",
            "<meta content=\"telephone=no\" name=\"format-detection\"/>\n",
            "<meta content=\"yes\" name=\"apple-touch-fullscreen\">\n",
            "<meta content=\"澎湃，Paper，The Paper，热问答，新闻跟踪，政治，时政，政经，澎湃新闻，新闻，思想，原创新闻，突发新闻，独家报道，上海报业，东方早报，东方报业，上海东方报业\" name=\"Keywords\"/>\n",
            "<meta content=\"澎湃，澎湃新闻，澎湃新闻网，新闻与思想，澎湃是植根于中国上海的时政思想类互联网平台，以最活跃的原创新闻与最冷静的思想分析为两翼，是互联网技术创新与新闻价值传承的结合体，致力于问答式新闻与新闻追踪功能的实践。\" name=\"Description\"/>\n",
            "<meta content=\"max-age=1700\" http-equiv=\"Cache-control\"/>\n",
            "<meta content=\"on\" http-equiv=\"cleartype\"/>\n",
            "<title>时差播客︱宗教学：信仰，魔法，身份，权力</title>\n",
            "<link href=\"https://file.thepaper.cn/wap/v6/css/reset.css?v=2.1.5\" rel=\"stylesheet\" type=\"text/css\"/>\n",
            "<link href=\"https://file.thepaper.cn/wap/v6/css/swiper-bundle.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
            "<link href=\"https://file.thepaper.cn/wap/v6/css/base_v6.css?v=2.1.5\" rel=\"stylesheet\" type=\"text/css\"/>\n",
            "<link href=\"https://file.thepaper.cn/wap/v6/css/homepage_v6.css?v=2.1.5\" rel=\"stylesheet\" type=\"text/css\"/>\n",
            "<link href=\"https://file.thepaper.cn/wap/v6/css/newsdetail_v6.css?v=2.1.5\" rel=\"stylesheet\" type=\"text/css\"/>\n",
            "<script src=\"//7b71.t4m.cn/applink.js\" type=\"text/jav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3:** Get the text with tags removal and without class selection\n",
        "\n",
        "To clean it up, we need to extract the content using **extract()** and **get_text()**. Now things look much better! Nonetheless, we still need to remove the header texts."
      ],
      "metadata": {
        "id": "2TgH1yEzTahw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kill all script and style elements\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "text = soup.get_text()"
      ],
      "metadata": {
        "id": "FFKVD9LCIKiW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[500:1500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQmg3Gf2Jh4M",
        "outputId": "187772b2-bb84-4188-f6a7-648ecb3ae4a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私家地理\n",
            "非常品\n",
            "楼市\n",
            "生活方式\n",
            "澎湃联播\n",
            "视界\n",
            "亲子学堂\n",
            "北京冬奥\n",
            "汽车圈\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "思想市场\n",
            "\n",
            "去APP听\n",
            "\n",
            "\n",
            "时差播客︱宗教学：信仰，魔法，身份，权力\n",
            "时差播客\n",
            "\n",
            "                2021-07-30 12:29 \n",
            "                \n",
            "                    \n",
            "                        来源：澎湃新闻\n",
            "                    \n",
            "                    \n",
            "                \n",
            "            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\r\n",
            "            {{newsTimeline.name}}\r\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\r\n",
            "                        {{item.occurrenceDay}}\r\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{{content.occurrenceTime}}\n",
            "\n",
            " {{content.name}}\n",
            "\n",
            "查看详情\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "全部展开\n",
            "收起时间线\n",
            "\n",
            "\n",
            "\n",
            "本期《时差》播客，主持人多伦多大学助理教授郭婷邀请到了来自宾夕法尼亚大学的程晓文教授、香港大学的李纪教授、弗吉尼亚理工大学的倪湛舸教授以及芝加哥大学神学院的神务硕士、医院宗教师郑利昕，以“宗教学：信仰，魔法，身份，权力”为题展开讨论。宗教并不外在于日常生活，而是弥散在社会、历史、文化、政治中的点滴；宗教学帮助我们反思历史，同时理解今天的世界。本文为时差播客与澎湃新闻合作刊发的文字稿，由澎湃新闻（www.thepaper.cn）记者龚思量整理。观音老母洞郭婷：在今天节目开始之前，我想先表达一下沉痛的悼念，前几天有一位宗教学界的前辈，台大的林富士老师（1960-2021）去世了。我本来并不是研究中国宗教的，也不研究传统的中国文史哲，所以并没有和林老师见过面。但我一直从他的研究中得到灵感，所以非常感谢他。这两天也在脸书上，看到很多他过去的同事和学生对他的纪念。虽然学术界很多时候是一个有失公正的地方，但还是有一些地方让人觉得温暖，就好像点亮了一盏灯，而那盏灯一直会亮下去。这一期我们来谈宗教学，不只是谈学界，也谈它的实践。在座几位虽然是跨学科的研究者或实践者，但也是宗教学出身。那我相信，大家在和别人介绍说自己研究宗教学的时候，通常会听到几个问题：\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4:** Get the text with tags removal and class selection\n",
        "\n",
        "We can remove all irrelevant sessions from the website by specifying the class. We can filter a specific class using **find()**. The class is identified using the developer tools in the browser. Please pay attention: we can only get the **first item** in this class using find(). Another option would be **find_all()** which returns a list of matches. The output is the clean text we were expecting.🌟"
      ],
      "metadata": {
        "id": "Gt1bzemOTmBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = soup.find(\"div\", {\"class\": \"newsdetail_content\"}).get_text()\n",
        "\n",
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "GmRBGXMkKDEj",
        "outputId": "fe6354df-fd8a-40b6-fc15-d9e60595e748"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'本期《时差》播客，主持人多伦多大学助理教授郭婷邀请到了来自宾夕法尼亚大学的程晓文教授、香港大学的李纪教授、弗吉尼亚理工大学的倪湛舸教授以及芝加哥大学神学院的神务硕士、医院宗教师郑利昕，以“宗教学：信仰，魔法，身份，权力”为题展开讨论。宗教并不外在于日常生活，而是弥散在社会、历史、文化、政治中的点滴；宗教学帮助我们反思历史，同时理解今天的世界。本文为时差播客与澎湃新闻合作刊发的文字稿，由澎湃新闻（www.thepaper.cn）记者龚思量整理。观音老母洞郭婷：在今天节目开始之前，我想先表达一下沉痛的悼念，前几天有一位宗教学界的前辈，台大的林富士老师（1960-2021）去世了。我本来并不是研究中国宗教的，也不研究传统的中国文史哲，所以并没有和林老师见过面。但我一直从他的研究中得到灵感，所以非常感谢他。这两天也在脸书上，看到很多他过去的同事和学生对他的纪念。虽然学术界很多时候是一个有失公正的地方，但还是有一些地方让人觉得温暖，就好像点亮了一盏灯，而那盏灯一直会亮下去。这一期我们来谈宗教学，不只是谈学界，也谈它的实践。在座几位虽然是跨学科的研究者或实践者，但也是宗教学出身。那我相信，大家在和别人介绍说自己研究宗教学的时候，通常会听到几个问题：一个是那你有没有宗教信仰？或者你研究哪一种宗教？以前还会听到的一个问题是，那你毕业之后做什么，是不是准备出家等等。我以前会开玩笑说，对，以后出家给人算命。其实不只是学界之外，包括学界之内，不同学科对宗教学领域都会有一些陌生，因为它确实是一个比较特殊的学科。就我自己而言，我博士的训练在爱丁堡大学的神学院。爱大神学院作为一个新兴科系，比较有抗争精神和创新精神。它设立之初就是为了和传统的神学或者是和宗教有关的学科对抗，所以它非常讲究世俗化和社会科学方法。我记得大部分宗教系的学者不论男女都打扮得非常不羁。在开会的时候，美国宗教学、尤其是圣经研究的学者尤其男性会打扮得非常闪亮，头发焗过、穿西装、带领带、鞋子都擦得很亮，但是英国宗教系的老师就穿得很随便。而宗教学学科的训练讲究宗教和社会的关系、宗教和当下社会的关系。虽然我当时的研究是从AI人工智能切入，但其实是研究是英国的世俗化的情况。当然，在神学院也会碰到其他科系的同学，比如有旧约研究、新约研究，神学研究，然后也有一些道学博士或者是教牧学的学位。那想请几位聊聊，你们的研究背景是怎么样的，也可以跟'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5:** Keyword Extraction\n",
        "**The final step is exactly what we did in the [last tutorial](https://pinkychow1010.github.io/digital-chinese-history-blog/level-4/chapter-4/jieba/text-mining/2022/02/05/Chinese_Keyword_Extraction_1.html)!** It defines the stopwords and extracts 10 keywords from the text."
      ],
      "metadata": {
        "id": "f5zjBGBvXQlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords= r\"/content/drive/MyDrive/NLP/stopwords.txt\"\n",
        "url = \"https://m.thepaper.cn/newsDetail_forward_16254733\"\n",
        "\n",
        "jieba.analyse.set_stop_words(stopwords)\n",
        "tags = jieba.analyse.extract_tags(text, topK=10, withWeight=True)"
      ],
      "metadata": {
        "id": "tn3dDW8aXHCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Keywords:**\n",
        "Let's look at our result."
      ],
      "metadata": {
        "id": "6jxHQmGzXe6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz4ma5wvXdXg",
        "outputId": "469e49f7-85ba-4deb-d22b-4693bdf207b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('宗教', 0.2162035113456979),\n",
              " ('研究', 0.1056884665277731),\n",
              " ('宗教学', 0.07345268363971678),\n",
              " ('基督教', 0.061966732508025965),\n",
              " ('女性', 0.05076592702813553),\n",
              " ('神学院', 0.04378034740564733),\n",
              " ('天主教', 0.040147481422144304),\n",
              " ('传统', 0.033391922322156105),\n",
              " ('现在', 0.032659074433310856),\n",
              " ('社会', 0.031786899511790284)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing a Function \n",
        "\n",
        "To simplify the steps, we can condense everything into a short function. If you do not know yet how to build a function, [check it out](https://pinkychow1010.github.io/digital-chinese-history-blog/programming/chapter-1/level-1/2020/01/26/FunctionsNLoops_Basics.html). This function will take an URL, the number of keywords and a stopword list. It will then return the keywords in the list. "
      ],
      "metadata": {
        "id": "tfhxk_nhG6aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(url,n,stopwords, withWeight=False):\n",
        "  \"\"\"\n",
        "  This function extract a number of keywords from a webpage after excluding the stopwords\n",
        "  url: str\n",
        "    the webpage\n",
        "  n: int\n",
        "    number of keywords extracted\n",
        "  stopwords: str\n",
        "    a path to the stopword text file\n",
        "  returns: list\n",
        "    list of keywords extracted from the webpage\n",
        "  \"\"\"\n",
        "  req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "  html = urlopen(req).read()\n",
        "  soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "  # kill all script and style elements\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()    # rip it out\n",
        "\n",
        "  # get text\n",
        "  text = soup.find(\"div\", {\"class\": \"newsdetail_content\"}).get_text()\n",
        "\n",
        "  # exclude stopwords\n",
        "  jieba.analyse.set_stop_words(stopwords)\n",
        "\n",
        "  # get keywords\n",
        "  tags = jieba.analyse.extract_tags(text, topK=n, withWeight=withWeight)\n",
        "  return tags"
      ],
      "metadata": {
        "id": "1WfeuX0-1JHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By applying the function, we get a list of 10 keywords:\n",
        "**'研究', '诗歌', '中国', '蔡宗齐', '学者', '澎湃', '文学', '诗境', '语法', '汉诗'**"
      ],
      "metadata": {
        "id": "2Xtldgl45UXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords= r\"/content/drive/MyDrive/NLP/stopwords.txt\"\n",
        "n = 10\n",
        "url = \"https://m.thepaper.cn/newsDetail_forward_16254733\"\n",
        "\n",
        "extract_keywords(url=url,n=n,stopwords=stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sypreaLM1sB_",
        "outputId": "ee3a8f54-f367-4584-e525-d99ec2c91c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['研究', '诗歌', '中国', '蔡宗齐', '学者', '澎湃', '文学', '诗境', '语法', '汉诗']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Try it out yourself** 🧐"
      ],
      "metadata": {
        "id": "ObEmdUo-zOIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords_general(url,n,stopwords, withWeight=False):\n",
        "  \"\"\"\n",
        "  This function extract a number of keywords from a webpage after excluding the stopwords\n",
        "  url: str\n",
        "    the webpage\n",
        "  n: int\n",
        "    number of keywords extracted\n",
        "  stopwords: str\n",
        "    a path to the stopword text file\n",
        "  returns: list\n",
        "    list of keywords extracted from the webpage\n",
        "  \"\"\"\n",
        "  req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "  html = urlopen(req).read()\n",
        "  soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "  # kill all script and style elements\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()    # rip it out\n",
        "\n",
        "  # get text\n",
        "  text = soup.get_text()\n",
        "\n",
        "  # exclude stopwords\n",
        "  jieba.analyse.set_stop_words(stopwords)\n",
        "\n",
        "  # get keywords\n",
        "  tags = jieba.analyse.extract_tags(text, topK=n, withWeight=withWeight)\n",
        "  return tags"
      ],
      "metadata": {
        "id": "j4GxeIDTy7Sc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤔 Put a URL here ⬇️"
      ],
      "metadata": {
        "id": "5AAwCc0t1Kri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords= r\"/content/drive/MyDrive/NLP/stopwords.txt\"\n",
        "\n",
        "myKeywords = extract_keywords_general(url=\"https://ctext.org/zh\",n=10,stopwords=stopwords) # Put in any URL you want\n",
        "print(myKeywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI7mSAqczJUp",
        "outputId": "c7132699-9715-4ef1-f0a4-809e76513140"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['資料', '顯示', '來源', '文獻', '字體', '中國', '圖書館', '這些', '本站', '算經']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Keywords from Multiple Blogs\n",
        "\n",
        "Until now, we can only extract keywords for one text at a time. To further automate what we did, we can loop through multiple articles. If you do not know yet how to build a loop, [check it out](https://pinkychow1010.github.io/digital-chinese-history-blog/programming/chapter-1/level-1/2020/01/26/FunctionsNLoops_Basics.html). Please pay attention: as we are going through the webpages using Python, the server might be overloaded with too many requests in a very short time. To avoid potential errors, we can catch the errors using **try** and **except**, and put **time.sleep()** in between using **time** library. We will let the program *sleep* for 5 seconds after scrapping each web address."
      ],
      "metadata": {
        "id": "GzMr7n4gHD0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords= r\"/content/drive/MyDrive/NLP/stopwords.txt\"\n",
        "n = 10\n",
        "keyword_list = []\n",
        "\n",
        "for page in range(10000000,10000010):\n",
        "  url = \"https://m.thepaper.cn/newsDetail_forward_{}\".format(page)\n",
        "  print(url)\n",
        "  time.sleep(5)\n",
        "\n",
        "  try:\n",
        "    keywords = extract_keywords(url=url,n=n,stopwords=stopwords)\n",
        "  except Exceptions:\n",
        "      print(\"Interrupted\")\n",
        "\n",
        "  keyword_list.append(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrDWRalnU8Zd",
        "outputId": "c1f29cd1-010b-4ffd-88fa-ca34b648b536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://m.thepaper.cn/newsDetail_forward_10000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.096 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://m.thepaper.cn/newsDetail_forward_10000001\n",
            "https://m.thepaper.cn/newsDetail_forward_10000002\n",
            "https://m.thepaper.cn/newsDetail_forward_10000003\n",
            "https://m.thepaper.cn/newsDetail_forward_10000004\n",
            "https://m.thepaper.cn/newsDetail_forward_10000005\n",
            "https://m.thepaper.cn/newsDetail_forward_10000006\n",
            "https://m.thepaper.cn/newsDetail_forward_10000007\n",
            "https://m.thepaper.cn/newsDetail_forward_10000008\n",
            "https://m.thepaper.cn/newsDetail_forward_10000009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Export Keyword List**\n",
        "\n",
        "We have appended the keywords to a list **keyword_list** when we loop through the articles. Now, we can access all keywords by looking into our list."
      ],
      "metadata": {
        "id": "tF82e6jL4r2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtbRGVE9MmWF",
        "outputId": "5b8d0ad2-d292-41fa-937d-f5ccc82d1890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['拔萝卜', '萝卜', '采摘', '孩子', '菜园', '收获', '周末版', '实践', '热爱劳动', '终觉'],\n",
              " ['绿色', '循环', '湖南省', '10', '环保', '生态', '活动', '兑换', '垃圾', '分类'],\n",
              " ['吴邮邮', '三轮车', '孩子', '小孙子', '事迹', '归仁', '2019', '12', '高文娟', '微信'],\n",
              " ['浙大', '浙江大学', '新人', '记者团', '缘定', '星河', '母校', '2020', '123', '李兰娟'],\n",
              " ['济南', '公安', '报告', '原文', '交警', '标题', '阅读'],\n",
              " ['年会', '环保', '湖南省', '社会', '行动者', '2020', '生态', '组织', '绿色', '环境治理'],\n",
              " ['斩肉', '海安', '炸制', '白斩', '炖煮', '--', '葱姜', '猴急', '麻虾', '黄毛'],\n",
              " ['高杰', '执法', '学法', '公安', '公安机关', '多面手', '全市', '法治', '复议', '民警'],\n",
              " ['栗子', '好钰', '炒栗子', '虹口', '小虹', '海宁路', '好好', '00', '野栗', '板栗'],\n",
              " ['海安', '博物馆', '陶瓷', '鸣谢', '匠心独运', '美轮美奂', '林裕翔', '邰颖', '喜欢', '光辉灿烂']]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also choose to put the list in a **Pandas** data frame and export it to a **csv** file. If you want to learn more about Pandas, check it out [here](https://pinkychow1010.github.io/digital-chinese-history-blog/level-2/chapter-2/data-manipulation/pandas/2020/01/23/Pandas_TextAnalysis_TextOriganization.html)."
      ],
      "metadata": {
        "id": "6Rx_opZh6i-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(keyword_list)"
      ],
      "metadata": {
        "id": "kv1tPp9xVXEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save each keyword into separate columns as strings."
      ],
      "metadata": {
        "id": "bLd_3agL7JFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna(value=np.nan).astype(str)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "mpkYTxayY_Fg",
        "outputId": "f866d9da-c947-4337-c0ad-f980482c9db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6e4a2242-7880-4b8a-a492-51e9c31dcf7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>拔萝卜</td>\n",
              "      <td>萝卜</td>\n",
              "      <td>采摘</td>\n",
              "      <td>孩子</td>\n",
              "      <td>菜园</td>\n",
              "      <td>收获</td>\n",
              "      <td>周末版</td>\n",
              "      <td>实践</td>\n",
              "      <td>热爱劳动</td>\n",
              "      <td>终觉</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>绿色</td>\n",
              "      <td>循环</td>\n",
              "      <td>湖南省</td>\n",
              "      <td>10</td>\n",
              "      <td>环保</td>\n",
              "      <td>生态</td>\n",
              "      <td>活动</td>\n",
              "      <td>兑换</td>\n",
              "      <td>垃圾</td>\n",
              "      <td>分类</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>吴邮邮</td>\n",
              "      <td>三轮车</td>\n",
              "      <td>孩子</td>\n",
              "      <td>小孙子</td>\n",
              "      <td>事迹</td>\n",
              "      <td>归仁</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>高文娟</td>\n",
              "      <td>微信</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>浙大</td>\n",
              "      <td>浙江大学</td>\n",
              "      <td>新人</td>\n",
              "      <td>记者团</td>\n",
              "      <td>缘定</td>\n",
              "      <td>星河</td>\n",
              "      <td>母校</td>\n",
              "      <td>2020</td>\n",
              "      <td>123</td>\n",
              "      <td>李兰娟</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>济南</td>\n",
              "      <td>公安</td>\n",
              "      <td>报告</td>\n",
              "      <td>原文</td>\n",
              "      <td>交警</td>\n",
              "      <td>标题</td>\n",
              "      <td>阅读</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>年会</td>\n",
              "      <td>环保</td>\n",
              "      <td>湖南省</td>\n",
              "      <td>社会</td>\n",
              "      <td>行动者</td>\n",
              "      <td>2020</td>\n",
              "      <td>生态</td>\n",
              "      <td>组织</td>\n",
              "      <td>绿色</td>\n",
              "      <td>环境治理</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>斩肉</td>\n",
              "      <td>海安</td>\n",
              "      <td>炸制</td>\n",
              "      <td>白斩</td>\n",
              "      <td>炖煮</td>\n",
              "      <td>--</td>\n",
              "      <td>葱姜</td>\n",
              "      <td>猴急</td>\n",
              "      <td>麻虾</td>\n",
              "      <td>黄毛</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>高杰</td>\n",
              "      <td>执法</td>\n",
              "      <td>学法</td>\n",
              "      <td>公安</td>\n",
              "      <td>公安机关</td>\n",
              "      <td>多面手</td>\n",
              "      <td>全市</td>\n",
              "      <td>法治</td>\n",
              "      <td>复议</td>\n",
              "      <td>民警</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>栗子</td>\n",
              "      <td>好钰</td>\n",
              "      <td>炒栗子</td>\n",
              "      <td>虹口</td>\n",
              "      <td>小虹</td>\n",
              "      <td>海宁路</td>\n",
              "      <td>好好</td>\n",
              "      <td>00</td>\n",
              "      <td>野栗</td>\n",
              "      <td>板栗</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>海安</td>\n",
              "      <td>博物馆</td>\n",
              "      <td>陶瓷</td>\n",
              "      <td>鸣谢</td>\n",
              "      <td>匠心独运</td>\n",
              "      <td>美轮美奂</td>\n",
              "      <td>林裕翔</td>\n",
              "      <td>邰颖</td>\n",
              "      <td>喜欢</td>\n",
              "      <td>光辉灿烂</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e4a2242-7880-4b8a-a492-51e9c31dcf7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e4a2242-7880-4b8a-a492-51e9c31dcf7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e4a2242-7880-4b8a-a492-51e9c31dcf7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     0     1    2    3     4     5     6     7     8     9\n",
              "0  拔萝卜    萝卜   采摘   孩子    菜园    收获   周末版    实践  热爱劳动    终觉\n",
              "1   绿色    循环  湖南省   10    环保    生态    活动    兑换    垃圾    分类\n",
              "2  吴邮邮   三轮车   孩子  小孙子    事迹    归仁  2019    12   高文娟    微信\n",
              "3   浙大  浙江大学   新人  记者团    缘定    星河    母校  2020   123   李兰娟\n",
              "4   济南    公安   报告   原文    交警    标题    阅读   nan   nan   nan\n",
              "5   年会    环保  湖南省   社会   行动者  2020    生态    组织    绿色  环境治理\n",
              "6   斩肉    海安   炸制   白斩    炖煮    --    葱姜    猴急    麻虾    黄毛\n",
              "7   高杰    执法   学法   公安  公安机关   多面手    全市    法治    复议    民警\n",
              "8   栗子    好钰  炒栗子   虹口    小虹   海宁路    好好    00    野栗    板栗\n",
              "9   海安   博物馆   陶瓷   鸣谢  匠心独运  美轮美奂   林裕翔    邰颖    喜欢  光辉灿烂"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also put all keywords together in a single column. It is done by applying function **join()** along axis 1 of our DataFrame."
      ],
      "metadata": {
        "id": "ur8mggs37Thf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = pd.DataFrame()\n",
        "df_join[\"keywords\"] = df.apply(lambda row: ','.join(row.values.astype(str)), axis=1)\n",
        "df_join"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "_TTOhJezWgvo",
        "outputId": "c81d9fac-db49-4268-c4f3-b0e08937b64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-79c1a951-e829-42b1-b84c-c0ebedc2fcc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>拔萝卜,萝卜,采摘,孩子,菜园,收获,周末版,实践,热爱劳动,终觉</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>绿色,循环,湖南省,10,环保,生态,活动,兑换,垃圾,分类</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>吴邮邮,三轮车,孩子,小孙子,事迹,归仁,2019,12,高文娟,微信</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>浙大,浙江大学,新人,记者团,缘定,星河,母校,2020,123,李兰娟</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>济南,公安,报告,原文,交警,标题,阅读,nan,nan,nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>年会,环保,湖南省,社会,行动者,2020,生态,组织,绿色,环境治理</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>斩肉,海安,炸制,白斩,炖煮,--,葱姜,猴急,麻虾,黄毛</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>高杰,执法,学法,公安,公安机关,多面手,全市,法治,复议,民警</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>栗子,好钰,炒栗子,虹口,小虹,海宁路,好好,00,野栗,板栗</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>海安,博物馆,陶瓷,鸣谢,匠心独运,美轮美奂,林裕翔,邰颖,喜欢,光辉灿烂</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79c1a951-e829-42b1-b84c-c0ebedc2fcc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79c1a951-e829-42b1-b84c-c0ebedc2fcc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79c1a951-e829-42b1-b84c-c0ebedc2fcc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                keywords\n",
              "0      拔萝卜,萝卜,采摘,孩子,菜园,收获,周末版,实践,热爱劳动,终觉\n",
              "1         绿色,循环,湖南省,10,环保,生态,活动,兑换,垃圾,分类\n",
              "2    吴邮邮,三轮车,孩子,小孙子,事迹,归仁,2019,12,高文娟,微信\n",
              "3   浙大,浙江大学,新人,记者团,缘定,星河,母校,2020,123,李兰娟\n",
              "4       济南,公安,报告,原文,交警,标题,阅读,nan,nan,nan\n",
              "5    年会,环保,湖南省,社会,行动者,2020,生态,组织,绿色,环境治理\n",
              "6          斩肉,海安,炸制,白斩,炖煮,--,葱姜,猴急,麻虾,黄毛\n",
              "7       高杰,执法,学法,公安,公安机关,多面手,全市,法治,复议,民警\n",
              "8        栗子,好钰,炒栗子,虹口,小虹,海宁路,好好,00,野栗,板栗\n",
              "9  海安,博物馆,陶瓷,鸣谢,匠心独运,美轮美奂,林裕翔,邰颖,喜欢,光辉灿烂"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download the data frame as a csv, we can use **to_csv()** and **files.download()**."
      ],
      "metadata": {
        "id": "jGkOnONf7vNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "df_join.to_csv('keywords.csv')\n",
        "files.download('keywords.csv')"
      ],
      "metadata": {
        "id": "6arv01Wf6tnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎉 \n",
        "Great! Now we have just learnt how to extract keywords from a webpage. This is only the basics to work with a simple webpage. To better understand the potential of BeautifulSoup, I recommand you to further search for BeautifulSoup tutorials on Youtube."
      ],
      "metadata": {
        "id": "weyUeNxp4CIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "***\n",
        "\n",
        "## **Additional information**\n",
        "\n",
        "This notebook is provided for educational purpose and feel free to report any issue on GitHub.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Author:** Ka Hei, Chow\n",
        "\n",
        "**License:** The code in this notebook is licensed under the [Creative Commons by Attribution 4.0 license](https://creativecommons.org/licenses/by/4.0/).\n",
        "\n",
        "**Last modified:** February 2022"
      ],
      "metadata": {
        "id": "lYN_MPOmnAdc"
      }
    }
  ]
}